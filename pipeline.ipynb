{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Adapted from https://github.com/GaelVaroquaux/scikit-learn-tutorial/blob/master/tutorial/working_with_text_data.rst\n",
      "made some changes to work with my data and also because that appears to be using an older version of scikit-learn. Some of the imports don't work."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import random\n",
      "import re\n",
      "from pandas import DataFrame\n",
      "import numpy as np\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn import metrics\n",
      "text_clf = Pipeline([\n",
      "    ('vect', CountVectorizer(min_df=1, stop_words='english', decode_error='ignore')),\n",
      "    ('tfidf', TfidfTransformer()),\n",
      "#    ('clf', MultinomialNB()),\n",
      "    ('clf', KNeighborsClassifier(n_neighbors=7))\n",
      "    ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = []\n",
      "label = []\n",
      "for eachLabel in ['dos','deface','losstheft','other']:\n",
      "    for eachFile in os.listdir(eachLabel):\n",
      "        fullPath = os.path.join(eachLabel,eachFile)\n",
      "        textFile = open(fullPath)\n",
      "        text.append(textFile.read())\n",
      "        label.append(eachLabel)\n",
      "\n",
      "# Now split our articles into three pools, a training set, a devtest set and a testing set.\n",
      "allData = DataFrame({'text':text,'label':label})\n",
      "allData = allData.reindex(np.random.permutation(allData.index))\n",
      "trainingData, devtestData, testingData = np.array_split(allData,3)\n",
      "print \"trainingData has %s dos articles.\" % len(trainingData[trainingData['label'] == 'dos'])\n",
      "print \"devtestData has %s dos articles.\" % len(devtestData[devtestData['label'] == 'dos'])\n",
      "print \"testingData has %s dos articles.\" % len(testingData[testingData['label'] == 'dos'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "trainingData has 38 dos articles.\n",
        "devtestData has 34 dos articles.\n",
        "testingData has 39 dos articles.\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = text_clf.fit(trainingData['text'],trainingData['label'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted = X_train.predict(testingData['text'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When examining the output of classification_report we can say that precision is the percentage of true positives. If the classifier says dos, how often is it really dos. The recall is measuring how many dos were correctly identified divided by how many dos there were to identify. So of all the DOS incidents in the trainingData, only 3% of them got flagged as DOS by the classifier. However, 100% of the articles flagged as DOS turned out to be DOS."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print metrics.classification_report(list(testingData['label']), list(predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "     deface       0.33      0.09      0.14        23\n",
        "        dos       1.00      0.03      0.05        39\n",
        "  losstheft       0.22      0.52      0.30       159\n",
        "      other       0.76      0.54      0.63       597\n",
        "\n",
        "avg / total       0.66      0.50      0.53       818\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# just looking at denial of service\n",
      "rightAnswers = 0\n",
      "allDosPredictions = 0\n",
      "for p, f in zip(predicted, testingData['label']):\n",
      "    if p == \"dos\":\n",
      "        allDosPredictions += 1\n",
      "        if p == f:\n",
      "            rightAnswers += 1\n",
      "print \"%s / %s\" % (rightAnswers, allDosPredictions)\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 / 1\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# experimenting with ngram_range=(1,3) and removing stop_words='english'\n",
      "text_clf = Pipeline([\n",
      "    ('vect', CountVectorizer(ngram_range=(1, 3), min_df=3, decode_error='ignore', lowercase=True)),\n",
      "    ('tfidf', TfidfTransformer()),\n",
      "    ('clf', LinearSVC()),\n",
      "    ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = text_clf.fit(trainingData['text'],trainingData['label'])\n",
      "predicted = X_train.predict(devtestData['text'])\n",
      "print metrics.classification_report(list(devtestData['label']), list(predicted))\n",
      "# Just another way of calculating this\n",
      "rightAnswers = 0\n",
      "rows = 0\n",
      "for p,t in zip(predicted, devtestData['label']):\n",
      "    if p == t: rightAnswers += 1\n",
      "    rows += 1\n",
      "print \"%s correct out of %s: %s\" % (rightAnswers, rows, float(rightAnswers)/rows)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "     deface       0.00      0.00      0.00        29\n",
        "        dos       1.00      0.04      0.08        46\n",
        "  losstheft       0.42      0.03      0.06       146\n",
        "      other       0.74      0.99      0.85       597\n",
        "\n",
        "avg / total       0.67      0.73      0.63       818\n",
        "\n",
        "599 correct out of 818: 0.732273838631\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train.predict([\"some asshole ddos my website\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "array(['other'], dtype=object)"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One thing I want to try is preprocessing the text to make some terms that mean the same thing get merged together. The function below will take several different ways of labeling a denial of service incident and make them all use the same term, ddos. __I can probably make this part of the build_preprocessor for countVectorizer__"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def commonizer(inString):\n",
      "    dosDashes = re.compile(r\"\\bdenial-of-service\", re.IGNORECASE)\n",
      "    dos = re.compile(r\"\\bdenial of service\", re.IGNORECASE)\n",
      "    ddos = re.compile(r\"\\bdistributed denial of service\", re.IGNORECASE)\n",
      "    ddosDashes = re.compile(r\"distributed denial-of-service\", re.IGNORECASE)\n",
      "    inString = re.sub(dosDashes, \"ddos\", inString)\n",
      "    inString = re.sub(dos, \"ddos\", inString)\n",
      "    inString = re.sub(ddos, \"ddos\", inString)\n",
      "    inString = re.sub(ddosDashes, \"ddos\", inString)\n",
      "    return inString\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So this is running the same pipeline as before but each article that gets fed to the classifier is first run through the commonizer function to make some terms consistent. I also made the new term a single token so that it wouldn't get split up. __The result seems to be a 2% increase in accuracy on denial of service incidents (recall column went from .42 to .44)__"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = text_clf.fit([commonizer(w) for w in trainingData['text']],trainingData['label'])\n",
      "predicted = X_train.predict(devtestData['text'])\n",
      "print metrics.classification_report(list(devtestData['label']), list(predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "     deface       0.00      0.00      0.00        29\n",
        "        dos       1.00      0.04      0.08        46\n",
        "  losstheft       0.38      0.03      0.06       146\n",
        "      other       0.74      0.99      0.84       597\n",
        "\n",
        "avg / total       0.66      0.73      0.63       818\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train.predict([\"some asshole ddos my website\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "array(['other'], dtype=object)"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}