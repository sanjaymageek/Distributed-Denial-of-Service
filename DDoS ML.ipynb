{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal here is to train a computer to recognize an article about Distributed Denial of Service attacks and put it into a queue of articles that need to be read or processed later. The source material is the VERIS Community Database (vcdb.org) which already has several dozen records on denial of service incidents and several associated news articles about each record."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import os\n",
      "from datetime import datetime\n",
      "import uuid\n",
      "import random\n",
      "from readability.readability import Document # pip install readability-xml\n",
      "import urllib\n",
      "import BeautifulSoup\n",
      "import nltk\n",
      "import nltk.classify.util\n",
      "from nltk.classify import NaiveBayesClassifier\n",
      "\n",
      "random.seed('follow @bfist on Twitter')\n",
      "vcdb_path = '/Users/v527234/Documents/development/python/vcdb/data/json'\n",
      "stopwords = nltk.corpus.stopwords.words('english')\n",
      "stopPunctuation = [',', ')', '(', '.', '#', '&', ';', '``', '$', \"''\", ':', '@']\n",
      "\n",
      "# i = getIncident('blahblahblah.json')\n",
      "def getIncident(inString):\n",
      "  return json.loads(open(inString).read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This is the feature extractor. This is the part that will pull stuff out of the article for analysis. In this iteration\n",
      "# we will just count whether or not a word is present.\n",
      "def featureExtractor(longString):\n",
      "    returnDict = {}\n",
      "    article_tokens = nltk.word_tokenize(longString)\n",
      "    article_tokens = [w.lower() for w in article_tokens]\n",
      "    article_tokens = [w for w in article_tokens if w not in stopwords]\n",
      "    article_tokens = [w for w in article_tokens if w not in stopPunctuation]\n",
      "    for eachWord in article_tokens:\n",
      "        returnDict[eachWord] = True\n",
      "    return returnDict\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "accuracy of the above extractor: 0.416932907348\n",
      "Most Informative Features\n",
      "       denial-of-service = True              DOS : NOT    =     93.8 : 1.0\n",
      "                    ddos = True              DOS : NOT    =     67.7 : 1.0\n",
      "                    gchq = True              DOS : NOT    =     46.0 : 1.0\n",
      "              cloudflare = True              DOS : NOT    =     46.0 : 1.0\n",
      "          hacktivists\u2019 = True              DOS : NOT    =     46.0 : 1.0\n",
      "             problematic = True              DOS : NOT    =     46.0 : 1.0\n",
      "                  pirate = True              DOS : NOT    =     46.0 : 1.0\n",
      "              disrupting = True              DOS : NOT    =     46.0 : 1.0\n",
      "               militates = True              DOS : NOT    =     46.0 : 1.0\n",
      "                   steam = True              DOS : NOT    =     46.0 : 1.0\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def featureExtractor2(longString):\n",
      "    returnDict = {}\n",
      "    if \"denial of service\" in longString.lower():\n",
      "        returnDict['denial of service'] = True\n",
      "    article_tokens = nltk.word_tokenize(longString)\n",
      "    article_tokens = [w.lower() for w in article_tokens]\n",
      "    article_tokens = [w for w in article_tokens if w not in stopwords]\n",
      "    article_tokens = [w for w in article_tokens if w not in stopPunctuation]\n",
      "    for eachWord in article_tokens:\n",
      "        returnDict[eachWord] = True\n",
      "    return returnDict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "accuracy: 0.447284345048\n",
      "Most Informative Features\n",
      "       denial-of-service = True              DOS : NOT    =    107.3 : 1.0\n",
      "                    ddos = True              DOS : NOT    =     88.2 : 1.0\n",
      "                   flood = True              DOS : NOT    =     64.4 : 1.0\n",
      "       denial of service = True              DOS : NOT    =     64.4 : 1.0\n",
      "                fighters = True              DOS : NOT    =     50.1 : 1.0\n",
      "             distributed = True              DOS : NOT    =     39.1 : 1.0\n",
      "           president\u2019s = True              DOS : NOT    =     35.8 : 1.0\n",
      "                protests = True              DOS : NOT    =     35.8 : 1.0\n",
      "         self-proclaimed = True              DOS : NOT    =     35.8 : 1.0\n",
      "              condemning = True              DOS : NOT    =     35.8 : 1.0\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def featureExtractor3(longString):\n",
      "    returnDict = {}\n",
      "    if \"denial of service\" in longString.lower():\n",
      "        returnDict['denial of service'] = True\n",
      "    article_tokens = nltk.word_tokenize(longString)\n",
      "    article_tokens = [w.lower() for w in article_tokens]\n",
      "    article_tokens = [w for w in article_tokens if w not in stopwords]\n",
      "    article_tokens = [w for w in article_tokens if w not in stopPunctuation]\n",
      "    wnl = nltk.WordNetLemmatizer()\n",
      "    article_tokens = [wnl.lemmatize(w) for w in article_tokens]\n",
      "    for eachWord in article_tokens:\n",
      "        returnDict[eachWord] = True\n",
      "    return returnDict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "accuracy: 0.47124600639\n",
      "Most Informative Features\n",
      "       denial-of-service = True              DOS : NOT    =     93.8 : 1.0\n",
      "                    ddos = True              DOS : NOT    =     67.7 : 1.0\n",
      "                    gchq = True              DOS : NOT    =     46.0 : 1.0\n",
      "              cloudflare = True              DOS : NOT    =     46.0 : 1.0\n",
      "          hacktivists\u2019 = True              DOS : NOT    =     46.0 : 1.0\n",
      "             problematic = True              DOS : NOT    =     46.0 : 1.0\n",
      "                  pirate = True              DOS : NOT    =     46.0 : 1.0\n",
      "              disrupting = True              DOS : NOT    =     46.0 : 1.0\n",
      "               militates = True              DOS : NOT    =     46.0 : 1.0\n",
      "                   steam = True              DOS : NOT    =     46.0 : 1.0\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get a list of articles about Denial of Service incidents\n",
      "dos_articles = []\n",
      "not_dos_articles = []\n",
      "for eachFile in os.listdir(vcdb_path):\n",
      "    fullName = os.path.join(vcdb_path, eachFile)\n",
      "    i = getIncident(fullName)\n",
      "    if 'DoS' in i['action'].get('hacking', {}).get('variety', []):\n",
      "        for article in i.get('reference', '').split(';'):\n",
      "            dos_articles.append(article)\n",
      "    else:\n",
      "        for article in i.get('reference', '').split(';'):\n",
      "            not_dos_articles.append(article)\n",
      "\n",
      "# print dos_articles[:10]\n",
      "# print not_dos_articles[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Clean the urls. Remove dates, and remove emtpy entries       \n",
      "dos_articles = [w.split(' ')[0] for w in dos_articles]\n",
      "dos_articles = [w for w in dos_articles if w != \"\"]\n",
      "for index, value in enumerate(dos_articles):\n",
      "    if value[-1] in [',', ';','?']:\n",
      "        dos_articles[index] = value[:-1]\n",
      "dos_articles = [w for w in dos_articles if not w.endswith('.pdf')]\n",
      "dos_articles = list(set(dos_articles))\n",
      "\n",
      "not_dos_articles = [w.split(' ')[0] for w in not_dos_articles]\n",
      "not_dos_articles = [w for w in not_dos_articles if w != \"\"]\n",
      "for index, value in enumerate(not_dos_articles):\n",
      "    if value[-1] in [',', ';','?']:\n",
      "        not_dos_articles[index] = value[:-1]\n",
      "not_dos_articles = [w for w in not_dos_articles if not w.endswith('.pdf')]\n",
      "not_dos_articles = list(set(not_dos_articles))\n",
      "\n",
      "\n",
      "print \"There are %s DOS articles.\" % len(dos_articles)\n",
      "print \"There are %s not DOS articles.\" % len(not_dos_articles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 101 DOS articles.\n",
        "There are 2049 not DOS articles.\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This is going to create some folders and download the content. It will take a while\n",
      "# if you've already done this you might want to leave this commented out\n",
      "#os.mkdir('dos')\n",
      "#os.mkdir('other')\n",
      "# counter = 0\n",
      "# for index, value in enumerate(dos_articles):\n",
      "#     try:\n",
      "#         article_text = urllib.urlopen(value).read()\n",
      "#         article_text = Document(article_text).summary()\n",
      "#         article_text = nltk.clean_html(article_text)\n",
      "#         article_text = article_text.encode('utf-8')\n",
      "#         outfile = open(os.path.join('./dos',str(index) +'.txt'), 'w')\n",
      "#         outfile.write(article_text)\n",
      "#         outfile.close()\n",
      "#         counter += 1\n",
      "#         if counter % 100 == 0:\n",
      "#             print \"Downloaded %s articles\" % counter\n",
      "#     except IOError:\n",
      "#         print \"Something wrong with index %s. Value: %s\" % (index,value)\n",
      "#     except AttributeError:\n",
      "#         print \"AttributeError on index %s. Value: %s\"  % (index,value)\n",
      "\n",
      "# for index, value in enumerate(not_dos_articles):\n",
      "#     try:\n",
      "#         article_text = urllib.urlopen(value).read()\n",
      "#         article_text = Document(article_text).summary()\n",
      "#         article_text = nltk.clean_html(article_text)\n",
      "#         article_text = article_text.encode('utf-8')\n",
      "#         outfile = open(os.path.join('./other',str(index).zfill(4) +'.txt'), 'w')\n",
      "#         outfile.write(article_text)\n",
      "#         outfile.close()\n",
      "#         counter += 1\n",
      "#         if counter % 100 == 0:\n",
      "#             print \"Downloaded %s articles\" % counter\n",
      "#     except:\n",
      "#         print \"Something wrong with index %s. Value: %s\" % (index,value)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need to create a list of tuples. Each tuple will contain a dictionary of features and a label of DOS or NOT"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_articles = []\n",
      "for eachFile in os.listdir('./dos'):\n",
      "    fullPath = os.path.join('./dos',eachFile)\n",
      "    textFile = open(fullPath)\n",
      "    all_articles.append((featureExtractor2(textFile.read()),'DOS'))\n",
      "for eachFile in os.listdir('./other'):\n",
      "    fullPath = os.path.join('./other', eachFile)\n",
      "    textFile = open(fullPath)\n",
      "    all_articles.append((featureExtractor2(textFile.read()),'NOT'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'featureExtractor2' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-21-3c08c78df553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfullPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./dos'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meachFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtextFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mall_articles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureExtractor2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DOS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meachFile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./other'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfullPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./other'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meachFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'featureExtractor2' is not defined"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now split our articles into three pools, a training set, a devtest set and a testing set.\n",
      "oneThird = len(all_articles)/3\n",
      "twoThird = oneThird*2\n",
      "random.shuffle(all_articles)\n",
      "training = all_articles[0:oneThird]\n",
      "devtest = all_articles[oneThird+1:twoThird]\n",
      "testing = all_articles[twoThird+1:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = NaiveBayesClassifier.train(training)\n",
      "print 'accuracy:', nltk.classify.util.accuracy(classifier, devtest)\n",
      "classifier.show_most_informative_features()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accuracy: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.416932907348\n",
        "Most Informative Features\n",
        "       denial-of-service = True              DOS : NOT    =     93.8 : 1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                    ddos = True              DOS : NOT    =     67.7 : 1.0\n",
        "                    gchq = True              DOS : NOT    =     46.0 : 1.0\n",
        "              cloudflare = True              DOS : NOT    =     46.0 : 1.0\n",
        "          hacktivists\u2019 = True              DOS : NOT    =     46.0 : 1.0\n",
        "             problematic = True              DOS : NOT    =     46.0 : 1.0\n",
        "                  pirate = True              DOS : NOT    =     46.0 : 1.0\n",
        "              disrupting = True              DOS : NOT    =     46.0 : 1.0\n",
        "               militates = True              DOS : NOT    =     46.0 : 1.0\n",
        "                   steam = True              DOS : NOT    =     46.0 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now I'm going to try something different\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vectorizer = CountVectorizer(min_df=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ddos_articles = [open(os.path.join(\"dos\",f)).read() for f in os.listdir(\"dos\")]\n",
      "other_articles = [open(os.path.join(\"other\",f)).read() for f in os.listdir(\"other\")]\n",
      "ddos_train = vectorizer.fit_transform(ddos_articles)\n",
      "other_train = vectorizer.fit_transform(other_articles)\n",
      "ddos_samples, ddos_features = ddos_train.shape\n",
      "other_samples, other_features = other_train.shape\n",
      "print(\"#samples: %d, #features: %d\" % (ddos_samples, ddos_features))\n",
      "print(\"#samples: %d, #features: %d\" % (other_samples, other_features))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 75, #features: 4004\n",
        "#samples: 1806, #features: 42599\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first thing I noticed is that the process of reading these articles and extracting the features went WAY faster using the technique above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_articles2 = []\n",
      "for eachFile in os.listdir('./dos'):\n",
      "    fullPath = os.path.join('./dos',eachFile)\n",
      "    textFile = open(fullPath)\n",
      "    all_articles2.append((textFile.read(),'DOS'))\n",
      "for eachFile in os.listdir('./other'):\n",
      "    fullPath = os.path.join('./other', eachFile)\n",
      "    textFile = open(fullPath)\n",
      "    all_articles2.append((textFile.read(),'NOT'))\n",
      "# Now split our articles into three pools, a training set, a devtest set and a testing set.\n",
      "oneThird = len(all_articles2)/3\n",
      "twoThird = oneThird*2\n",
      "random.shuffle(all_articles)\n",
      "training = all_articles2[0:oneThird]\n",
      "devtest = all_articles2[oneThird+1:twoThird]\n",
      "testing = all_articles2[twoThird+1:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
      "article2_text = [r[0] for r in training]\n",
      "ddos_train = vectorizer.fit_transform(article2_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "def dist(v1, v2):\n",
      "    delta = v1-v2\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "new_article_vector = vectorizer.transform(devtest[0][0].decode(errors='ignore').encode('utf-8'))\n",
      "\n",
      "print(new_article_vector.get_shape())\n",
      "print(ddos_train.getrow(0).get_shape())\n",
      "\n",
      "for index, text in enumerate(training):\n",
      "    article_vector = ddos_train.getrow(index)\n",
      "    d = dist(article_vector, new_article_vector)\n",
      "    print(\"Post %i has dist $.2f\" % (index,d))\n",
      "             "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "inconsistent shapes",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-55-b28cfbaa7bcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0marticle_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddos_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_article_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Post %i has dist $.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-55-b28cfbaa7bcf>\u001b[0m in \u001b[0;36mdist\u001b[0;34m(v1, v2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/v527234/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36m__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inconsistent shapes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_binopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'_minus_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: inconsistent shapes"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(2520, 18805)\n",
        "(1, 18805)\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "devtest[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "'&#13;\\n\\t\\t\\t\\t\\t Portia Williams PDT Staff Writer COVINGTON, Ky. \\xe2\\x80\\x94 The office of Kerry B. Harvey the United States Attorney Eastern District of Kentucky is reporting this week that Brenda Gammon, 41, of South Shore, Ky., admitted in federal court to bank embezzlement and aggravated identity theft charges on Dec. 10. According to the US Attorney\\xe2\\x80\\x99s office, Gammon pleaded guilty on Tuesday to an information charging her with bank embezzlement and aggravated identity theft before U.S. District Court Judge David L. Bunning. Gammon admitted to embezzlement of $829,600 during a time period of June 28, 2004, through July 18, 2012. Gammon worked as a teller, and later head teller, at the South Shore branch of Home Federal Savings and Loan, where the embezzlement took place. Gammon further admitted that as a teller, she had access to the account and balance totals of customer accounts, and how frequently they accessed those accounts. Using accounts that had significant deposits, Gammon would reportedly apply for share loans in those customers\\xe2\\x80\\x99 names, bypass the loan approval process, and approve the loans herself. She would then have checks issued and cash the checks herself. While no individual customer lost any money, South Shore Home Federal Savings and Loan did lose $829,600. According to the US Attorney\\xe2\\x80\\x99s Eastern District, as part of her plea, Gammon agreed to pay restitution in the amount of $829,600.00. The aggravated identity theft count was based on Gammon using the identity of real customers in obtaining the money. The investigation was conducted by Federal Bureau of Investigation. The U.S. Attorney\\xe2\\x80\\x99s Office was represented by Assistant U.S. Attorney Edwin J. Walbourn, III. Kerry B. Harvey, United States Attorney for the Eastern District of Kentucky, and Perrye Turner, Special Agent In Charge, Federal Bureau of Investigation, jointly announced the guilty plea this week. Gammon is scheduled to be sentenced on April 14, 2014. She faces up to 30 years in prison and a maximum fine of $250,000.00. Regarding the aggravated identity theft charge, a mandatory sentence of two years, consecutive to the sentence for embezzlement, is required. Bunning allowed Gammon to remain free on her own recognizance pending sentencing, which is scheduled for April 14, 2014. Gammon faces up to 30 years in prison and a maximum fine of $250,000. Portia Williams may be reached at 740-353-3101, ext. 286 or portiawilliams@civitasmedia.com. For breaking news, follow Portia on Twitter @PortiaWillPDT. &#13;\\n\\t\\t\\t\\t &#13;'"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}